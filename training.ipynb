{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bad': 0, 'good': 1}\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/spectrograms' #looking in subfolder train\n",
    "dataset = datasets.ImageFolder(root=data_path,transform=transforms.Compose([transforms.Grayscale(),transforms.ToTensor()]))\n",
    "class_map=dataset.class_to_idx\n",
    "print(class_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 4665\n",
      "Testing size: 1167\n"
     ]
    }
   ],
   "source": [
    "#split data to test and train\n",
    "#use 80% to train\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Training size:\", len(train_dataset))\n",
    "print(\"Testing size:\",len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNNet                                    [25, 2]                   --\n",
       "├─Sequential: 1-1                        [25, 64, 43, 7]           --\n",
       "│    └─Conv2d: 2-1                       [25, 8, 2809, 508]        208\n",
       "│    └─ReLU: 2-2                         [25, 8, 2809, 508]        --\n",
       "│    └─BatchNorm2d: 2-3                  [25, 8, 2809, 508]        16\n",
       "│    └─Conv2d: 2-4                       [25, 16, 2807, 506]       1,168\n",
       "│    └─MaxPool2d: 2-5                    [25, 16, 1403, 253]       --\n",
       "│    └─ReLU: 2-6                         [25, 16, 1403, 253]       --\n",
       "│    └─BatchNorm2d: 2-7                  [25, 16, 1403, 253]       32\n",
       "│    └─Conv2d: 2-8                       [25, 32, 1401, 251]       4,640\n",
       "│    └─Dropout2d: 2-9                    [25, 32, 1401, 251]       --\n",
       "│    └─MaxPool2d: 2-10                   [25, 32, 700, 125]        --\n",
       "│    └─ReLU: 2-11                        [25, 32, 700, 125]        --\n",
       "│    └─BatchNorm2d: 2-12                 [25, 32, 700, 125]        64\n",
       "│    └─Conv2d: 2-13                      [25, 64, 698, 123]        18,496\n",
       "│    └─Dropout2d: 2-14                   [25, 64, 698, 123]        --\n",
       "│    └─MaxPool2d: 2-15                   [25, 64, 349, 61]         --\n",
       "│    └─ReLU: 2-16                        [25, 64, 349, 61]         --\n",
       "│    └─BatchNorm2d: 2-17                 [25, 64, 349, 61]         128\n",
       "│    └─Conv2d: 2-18                      [25, 64, 347, 59]         36,928\n",
       "│    └─Dropout2d: 2-19                   [25, 64, 347, 59]         --\n",
       "│    └─MaxPool2d: 2-20                   [25, 64, 173, 29]         --\n",
       "│    └─ReLU: 2-21                        [25, 64, 173, 29]         --\n",
       "│    └─BatchNorm2d: 2-22                 [25, 64, 173, 29]         128\n",
       "│    └─Conv2d: 2-23                      [25, 64, 86, 14]          36,928\n",
       "│    └─Dropout2d: 2-24                   [25, 64, 86, 14]          --\n",
       "│    └─MaxPool2d: 2-25                   [25, 64, 43, 7]           --\n",
       "│    └─ReLU: 2-26                        [25, 64, 43, 7]           --\n",
       "│    └─BatchNorm2d: 2-27                 [25, 64, 43, 7]           128\n",
       "├─Flatten: 1-2                           [25, 19264]               --\n",
       "├─Linear: 1-3                            [25, 50]                  963,250\n",
       "├─Linear: 1-4                            [25, 2]                   102\n",
       "==========================================================================================\n",
       "Total params: 1,062,216\n",
       "Trainable params: 1,062,216\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 149.42\n",
       "==========================================================================================\n",
       "Input size (MB): 144.03\n",
       "Forward/backward pass size (MB): 14774.82\n",
       "Params size (MB): 4.25\n",
       "Estimated Total Size (MB): 14923.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNNet (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1,self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        self.drop2 = nn.Dropout2d()\n",
    "        self.pool2=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv2, self.pool2, self.relu2,self.bn2]\n",
    "\n",
    "        # third Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        self.drop3 = nn.Dropout2d()\n",
    "        self.pool3=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv3, self.drop3, self.pool3,self.relu3,self.bn3]\n",
    "\n",
    "        # fourth Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        nn.init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        self.drop4 = nn.Dropout2d()\n",
    "        self.pool4=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv4, self.drop4, self.pool4,self.relu4,self.bn4]\n",
    "\n",
    "        # 5 Block\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        nn.init.kaiming_normal_(self.conv5.weight, a=0.1)\n",
    "        self.conv5.bias.data.zero_()\n",
    "        self.drop5 = nn.Dropout2d()\n",
    "        self.pool5=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv5, self.drop5, self.pool5,self.relu5,self.bn5]\n",
    "\n",
    "        # Block 6\n",
    "        self.conv6 = nn.Conv2d(64, 64, kernel_size=3, stride=(2, 2))\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "        nn.init.kaiming_normal_(self.conv6.weight, a=0.1)\n",
    "        self.conv6.bias.data.zero_()\n",
    "        self.drop6 = nn.Dropout2d()\n",
    "        self.pool6=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv6, self.drop6, self.pool6,self.relu6,self.bn6]\n",
    "\n",
    "        self.flatten=nn.Flatten()\n",
    "        # Linear Classifier\n",
    "        self.lin1 = nn.Linear(19264,50)\n",
    "        self.lin2=nn.Linear(50,2)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x=self.flatten(x)\n",
    "\n",
    "        # Linear layer\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x=self.lin2(x)\n",
    "\n",
    "        # Final output\n",
    "        x=F.log_softmax(x,dim=1)\n",
    "        return x\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = CNNet()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device\n",
    "\n",
    "summary(myModel, input_size=(25,1,2813,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(myModel.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training function\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        \n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 60 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "\n",
    "# Create the validation/test function\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "\n",
    "    print(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.681952  [    0/ 4665]\n",
      "loss: 0.089385  [  900/ 4665]\n",
      "loss: 0.209835  [ 1800/ 4665]\n",
      "loss: 0.175748  [ 2700/ 4665]\n",
      "loss: 0.099023  [ 3600/ 4665]\n",
      "loss: 0.061722  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 96.7%, avg loss: 0.003716\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.074386  [    0/ 4665]\n",
      "loss: 0.004042  [  900/ 4665]\n",
      "loss: 0.017349  [ 1800/ 4665]\n",
      "loss: 0.007759  [ 2700/ 4665]\n",
      "loss: 0.005394  [ 3600/ 4665]\n",
      "loss: 0.029006  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 96.3%, avg loss: 0.006227\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.030041  [    0/ 4665]\n",
      "loss: 0.057242  [  900/ 4665]\n",
      "loss: 0.032185  [ 1800/ 4665]\n",
      "loss: 0.050162  [ 2700/ 4665]\n",
      "loss: 0.001928  [ 3600/ 4665]\n",
      "loss: 0.069080  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.1%, avg loss: 0.002474\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.017226  [    0/ 4665]\n",
      "loss: 0.000460  [  900/ 4665]\n",
      "loss: 0.023194  [ 1800/ 4665]\n",
      "loss: 0.000602  [ 2700/ 4665]\n",
      "loss: 0.000846  [ 3600/ 4665]\n",
      "loss: 0.037333  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.2%, avg loss: 0.001781\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.022173  [    0/ 4665]\n",
      "loss: 0.000552  [  900/ 4665]\n",
      "loss: 0.002199  [ 1800/ 4665]\n",
      "loss: 0.028767  [ 2700/ 4665]\n",
      "loss: 0.001559  [ 3600/ 4665]\n",
      "loss: 0.081073  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.6%, avg loss: 0.001383\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.003580  [    0/ 4665]\n",
      "loss: 0.012567  [  900/ 4665]\n",
      "loss: 0.001961  [ 1800/ 4665]\n",
      "loss: 0.005047  [ 2700/ 4665]\n",
      "loss: 0.117667  [ 3600/ 4665]\n",
      "loss: 0.000527  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.6%, avg loss: 0.001144\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.008930  [    0/ 4665]\n",
      "loss: 0.000983  [  900/ 4665]\n",
      "loss: 0.045865  [ 1800/ 4665]\n",
      "loss: 0.000239  [ 2700/ 4665]\n",
      "loss: 0.001933  [ 3600/ 4665]\n",
      "loss: 0.003726  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.8%, avg loss: 0.000825\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.001719  [    0/ 4665]\n",
      "loss: 0.006973  [  900/ 4665]\n",
      "loss: 0.010111  [ 1800/ 4665]\n",
      "loss: 0.000679  [ 2700/ 4665]\n",
      "loss: 0.000782  [ 3600/ 4665]\n",
      "loss: 0.000242  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.7%, avg loss: 0.000462\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.013609  [    0/ 4665]\n",
      "loss: 0.115533  [  900/ 4665]\n",
      "loss: 0.000319  [ 1800/ 4665]\n",
      "loss: 0.001218  [ 2700/ 4665]\n",
      "loss: 0.000639  [ 3600/ 4665]\n",
      "loss: 0.004325  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.8%, avg loss: 0.000321\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.027083  [    0/ 4665]\n",
      "loss: 0.000012  [  900/ 4665]\n",
      "loss: 0.000659  [ 1800/ 4665]\n",
      "loss: 0.071931  [ 2700/ 4665]\n",
      "loss: 0.015801  [ 3600/ 4665]\n",
      "loss: 0.000710  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.6%, avg loss: 0.000508\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.000674  [    0/ 4665]\n",
      "loss: 0.007137  [  900/ 4665]\n",
      "loss: 0.000018  [ 1800/ 4665]\n",
      "loss: 0.000096  [ 2700/ 4665]\n",
      "loss: 0.000103  [ 3600/ 4665]\n",
      "loss: 0.022726  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.8%, avg loss: 0.001075\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.000136  [    0/ 4665]\n",
      "loss: 0.079482  [  900/ 4665]\n",
      "loss: 0.000574  [ 1800/ 4665]\n",
      "loss: 0.000463  [ 2700/ 4665]\n",
      "loss: 0.001808  [ 3600/ 4665]\n",
      "loss: 0.000023  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 100.0%, avg loss: 0.000646\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.013604  [    0/ 4665]\n",
      "loss: 0.000002  [  900/ 4665]\n",
      "loss: 0.000122  [ 1800/ 4665]\n",
      "loss: 0.000463  [ 2700/ 4665]\n",
      "loss: 0.000786  [ 3600/ 4665]\n",
      "loss: 0.000151  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.7%, avg loss: 0.000289\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.000106  [    0/ 4665]\n",
      "loss: 0.001372  [  900/ 4665]\n",
      "loss: 0.001372  [ 1800/ 4665]\n",
      "loss: 0.129549  [ 2700/ 4665]\n",
      "loss: 0.193860  [ 3600/ 4665]\n",
      "loss: 0.160242  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.8%, avg loss: 0.000264\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.000060  [    0/ 4665]\n",
      "loss: 0.000022  [  900/ 4665]\n",
      "loss: 0.000441  [ 1800/ 4665]\n",
      "loss: 0.001720  [ 2700/ 4665]\n",
      "loss: 0.000167  [ 3600/ 4665]\n",
      "loss: 0.007293  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.7%, avg loss: 0.000407\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "epochs = 15\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    train(train_dataloader, myModel, cost, optimizer)\n",
    "    test(test_dataloader, myModel)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\multiprocessing\\queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"C:\\Python311\\Lib\\multiprocessing\\connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"C:\\Python311\\Lib\\multiprocessing\\connection.py\", line 276, in _close\n",
      "    _CloseHandle(self._handle)\n",
      "OSError: [WinError 6] The handle is invalid\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\stuff\\random2\\training.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/stuff/random2/training.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test_loss, correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/stuff/random2/training.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m reverse_class_map \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mbad\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgood\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/stuff/random2/training.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m test(train_dataloader,myModel)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/stuff/random2/training.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m test(test_dataloader,myModel)\n",
      "\u001b[1;32mc:\\stuff\\random2\\training.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/stuff/random2/training.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m test_loss, correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/stuff/random2/training.ipynb#X11sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/stuff/random2/training.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch, (X, Y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/stuff/random2/training.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         X, Y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), Y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/stuff/random2/training.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         pred \u001b[39m=\u001b[39m model(X)\n",
      "File \u001b[1;32mc:\\stuff\\random2\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\stuff\\random2\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\stuff\\random2\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1035\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1044\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\multiprocessing\\popen_spawn_win32.py:94\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 94\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     95\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    self.run()\n",
      "  File \"C:\\Python311\\Lib\\threading.py\", line 975, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Python311\\Lib\\multiprocessing\\queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "myModel.eval()\n",
    "test_loss, correct = 0, 0\n",
    "reverse_class_map = ['bad', 'good']\n",
    "\n",
    "test(train_dataloader,myModel)\n",
    "test(test_dataloader,myModel)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': 0}\n",
      "('./data/spectrograms/test\\\\test\\\\01 Mountain and River Map_0.png', 0) Predicted:=bad, Solution= 1\n",
      "('./data/spectrograms/test\\\\test\\\\01 Mountain and River Map_1.png', 0) Predicted:=bad, Solution= 1\n",
      "('./data/spectrograms/test\\\\test\\\\b_0.png', 0) Predicted:=good, Solution= 1\n",
      "('./data/spectrograms/test\\\\test\\\\b_1.png', 0) Predicted:=good, Solution= 1\n",
      "('./data/spectrograms/test\\\\test\\\\c_0.png', 0) Predicted:=good, Solution= 1\n",
      "('./data/spectrograms/test\\\\test\\\\c_1.png', 0) Predicted:=good, Solution= 1\n",
      "('./data/spectrograms/test\\\\test\\\\murda_0.png', 0) Predicted:=bad, Solution= 1\n",
      "('./data/spectrograms/test\\\\test\\\\murda_1.png', 0) Predicted:=bad, Solution= 1\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
