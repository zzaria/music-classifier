{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bad': 0, 'good': 1}\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/spectrograms' #looking in subfolder train\n",
    "dataset = datasets.ImageFolder(root=data_path,transform=transforms.Compose([transforms.Grayscale(),transforms.ToTensor()]))\n",
    "class_map=dataset.class_to_idx\n",
    "print(class_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 4665\n",
      "Testing size: 1167\n"
     ]
    }
   ],
   "source": [
    "#split data to test and train\n",
    "#use 80% to train\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Training size:\", len(train_dataset))\n",
    "print(\"Testing size:\",len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=50,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNNet                                    [25, 2]                   --\n",
       "├─Sequential: 1-1                        [25, 64, 43, 7]           --\n",
       "│    └─Conv2d: 2-1                       [25, 8, 2809, 508]        208\n",
       "│    └─ReLU: 2-2                         [25, 8, 2809, 508]        --\n",
       "│    └─BatchNorm2d: 2-3                  [25, 8, 2809, 508]        16\n",
       "│    └─Conv2d: 2-4                       [25, 16, 2807, 506]       1,168\n",
       "│    └─MaxPool2d: 2-5                    [25, 16, 1403, 253]       --\n",
       "│    └─ReLU: 2-6                         [25, 16, 1403, 253]       --\n",
       "│    └─BatchNorm2d: 2-7                  [25, 16, 1403, 253]       32\n",
       "│    └─Conv2d: 2-8                       [25, 32, 1401, 251]       4,640\n",
       "│    └─Dropout2d: 2-9                    [25, 32, 1401, 251]       --\n",
       "│    └─MaxPool2d: 2-10                   [25, 32, 700, 125]        --\n",
       "│    └─ReLU: 2-11                        [25, 32, 700, 125]        --\n",
       "│    └─BatchNorm2d: 2-12                 [25, 32, 700, 125]        64\n",
       "│    └─Conv2d: 2-13                      [25, 64, 698, 123]        18,496\n",
       "│    └─Dropout2d: 2-14                   [25, 64, 698, 123]        --\n",
       "│    └─MaxPool2d: 2-15                   [25, 64, 349, 61]         --\n",
       "│    └─ReLU: 2-16                        [25, 64, 349, 61]         --\n",
       "│    └─BatchNorm2d: 2-17                 [25, 64, 349, 61]         128\n",
       "│    └─Conv2d: 2-18                      [25, 64, 347, 59]         36,928\n",
       "│    └─Dropout2d: 2-19                   [25, 64, 347, 59]         --\n",
       "│    └─MaxPool2d: 2-20                   [25, 64, 173, 29]         --\n",
       "│    └─ReLU: 2-21                        [25, 64, 173, 29]         --\n",
       "│    └─BatchNorm2d: 2-22                 [25, 64, 173, 29]         128\n",
       "│    └─Conv2d: 2-23                      [25, 64, 86, 14]          36,928\n",
       "│    └─Dropout2d: 2-24                   [25, 64, 86, 14]          --\n",
       "│    └─MaxPool2d: 2-25                   [25, 64, 43, 7]           --\n",
       "│    └─ReLU: 2-26                        [25, 64, 43, 7]           --\n",
       "│    └─BatchNorm2d: 2-27                 [25, 64, 43, 7]           128\n",
       "├─Flatten: 1-2                           [25, 19264]               --\n",
       "├─Linear: 1-3                            [25, 50]                  963,250\n",
       "├─Linear: 1-4                            [25, 2]                   102\n",
       "==========================================================================================\n",
       "Total params: 1,062,216\n",
       "Trainable params: 1,062,216\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 149.42\n",
       "==========================================================================================\n",
       "Input size (MB): 144.03\n",
       "Forward/backward pass size (MB): 14774.82\n",
       "Params size (MB): 4.25\n",
       "Estimated Total Size (MB): 14923.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNNet (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1,self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        self.drop2 = nn.Dropout2d()\n",
    "        self.pool2=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv2, self.pool2, self.relu2,self.bn2]\n",
    "\n",
    "        # third Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        self.drop3 = nn.Dropout2d()\n",
    "        self.pool3=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv3, self.drop3, self.pool3,self.relu3,self.bn3]\n",
    "\n",
    "        # fourth Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        nn.init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        self.drop4 = nn.Dropout2d()\n",
    "        self.pool4=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv4, self.drop4, self.pool4,self.relu4,self.bn4]\n",
    "\n",
    "        # 5 Block\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        nn.init.kaiming_normal_(self.conv5.weight, a=0.1)\n",
    "        self.conv5.bias.data.zero_()\n",
    "        self.drop5 = nn.Dropout2d()\n",
    "        self.pool5=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv5, self.drop5, self.pool5,self.relu5,self.bn5]\n",
    "\n",
    "        # Block 6\n",
    "        self.conv6 = nn.Conv2d(64, 64, kernel_size=3, stride=(2, 2))\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "        nn.init.kaiming_normal_(self.conv6.weight, a=0.1)\n",
    "        self.conv6.bias.data.zero_()\n",
    "        self.drop6 = nn.Dropout2d()\n",
    "        self.pool6=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv6, self.drop6, self.pool6,self.relu6,self.bn6]\n",
    "\n",
    "        self.flatten=nn.Flatten()\n",
    "        # Linear Classifier\n",
    "        self.lin1 = nn.Linear(19264,50)\n",
    "        self.lin2=nn.Linear(50,2)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x=self.flatten(x)\n",
    "\n",
    "        # Linear layer\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x=self.lin2(x)\n",
    "\n",
    "        # Final output\n",
    "        x=F.log_softmax(x,dim=1)\n",
    "        return x\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = CNNet()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device\n",
    "\n",
    "summary(myModel, input_size=(25,1,2813,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(myModel.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training function\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        \n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 60 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "\n",
    "# Create the validation/test function\n",
    "\n",
    "def test(dataloader, model):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    correct /= len(dataloader.dataset)\n",
    "\n",
    "    print(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.681952  [    0/ 4665]\n",
      "loss: 0.089385  [  900/ 4665]\n",
      "loss: 0.209835  [ 1800/ 4665]\n",
      "loss: 0.175748  [ 2700/ 4665]\n",
      "loss: 0.099023  [ 3600/ 4665]\n",
      "loss: 0.061722  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 96.7%, avg loss: 0.003716\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.074386  [    0/ 4665]\n",
      "loss: 0.004042  [  900/ 4665]\n",
      "loss: 0.017349  [ 1800/ 4665]\n",
      "loss: 0.007759  [ 2700/ 4665]\n",
      "loss: 0.005394  [ 3600/ 4665]\n",
      "loss: 0.029006  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 96.3%, avg loss: 0.006227\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.030041  [    0/ 4665]\n",
      "loss: 0.057242  [  900/ 4665]\n",
      "loss: 0.032185  [ 1800/ 4665]\n",
      "loss: 0.050162  [ 2700/ 4665]\n",
      "loss: 0.001928  [ 3600/ 4665]\n",
      "loss: 0.069080  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.1%, avg loss: 0.002474\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.017226  [    0/ 4665]\n",
      "loss: 0.000460  [  900/ 4665]\n",
      "loss: 0.023194  [ 1800/ 4665]\n",
      "loss: 0.000602  [ 2700/ 4665]\n",
      "loss: 0.000846  [ 3600/ 4665]\n",
      "loss: 0.037333  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.2%, avg loss: 0.001781\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.022173  [    0/ 4665]\n",
      "loss: 0.000552  [  900/ 4665]\n",
      "loss: 0.002199  [ 1800/ 4665]\n",
      "loss: 0.028767  [ 2700/ 4665]\n",
      "loss: 0.001559  [ 3600/ 4665]\n",
      "loss: 0.081073  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.6%, avg loss: 0.001383\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.003580  [    0/ 4665]\n",
      "loss: 0.012567  [  900/ 4665]\n",
      "loss: 0.001961  [ 1800/ 4665]\n",
      "loss: 0.005047  [ 2700/ 4665]\n",
      "loss: 0.117667  [ 3600/ 4665]\n",
      "loss: 0.000527  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.6%, avg loss: 0.001144\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.008930  [    0/ 4665]\n",
      "loss: 0.000983  [  900/ 4665]\n",
      "loss: 0.045865  [ 1800/ 4665]\n",
      "loss: 0.000239  [ 2700/ 4665]\n",
      "loss: 0.001933  [ 3600/ 4665]\n",
      "loss: 0.003726  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.8%, avg loss: 0.000825\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.001719  [    0/ 4665]\n",
      "loss: 0.006973  [  900/ 4665]\n",
      "loss: 0.010111  [ 1800/ 4665]\n",
      "loss: 0.000679  [ 2700/ 4665]\n",
      "loss: 0.000782  [ 3600/ 4665]\n",
      "loss: 0.000242  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.7%, avg loss: 0.000462\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.013609  [    0/ 4665]\n",
      "loss: 0.115533  [  900/ 4665]\n",
      "loss: 0.000319  [ 1800/ 4665]\n",
      "loss: 0.001218  [ 2700/ 4665]\n",
      "loss: 0.000639  [ 3600/ 4665]\n",
      "loss: 0.004325  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.8%, avg loss: 0.000321\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.027083  [    0/ 4665]\n",
      "loss: 0.000012  [  900/ 4665]\n",
      "loss: 0.000659  [ 1800/ 4665]\n",
      "loss: 0.071931  [ 2700/ 4665]\n",
      "loss: 0.015801  [ 3600/ 4665]\n",
      "loss: 0.000710  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.6%, avg loss: 0.000508\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.000674  [    0/ 4665]\n",
      "loss: 0.007137  [  900/ 4665]\n",
      "loss: 0.000018  [ 1800/ 4665]\n",
      "loss: 0.000096  [ 2700/ 4665]\n",
      "loss: 0.000103  [ 3600/ 4665]\n",
      "loss: 0.022726  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.8%, avg loss: 0.001075\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.000136  [    0/ 4665]\n",
      "loss: 0.079482  [  900/ 4665]\n",
      "loss: 0.000574  [ 1800/ 4665]\n",
      "loss: 0.000463  [ 2700/ 4665]\n",
      "loss: 0.001808  [ 3600/ 4665]\n",
      "loss: 0.000023  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 100.0%, avg loss: 0.000646\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.013604  [    0/ 4665]\n",
      "loss: 0.000002  [  900/ 4665]\n",
      "loss: 0.000122  [ 1800/ 4665]\n",
      "loss: 0.000463  [ 2700/ 4665]\n",
      "loss: 0.000786  [ 3600/ 4665]\n",
      "loss: 0.000151  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.7%, avg loss: 0.000289\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.000106  [    0/ 4665]\n",
      "loss: 0.001372  [  900/ 4665]\n",
      "loss: 0.001372  [ 1800/ 4665]\n",
      "loss: 0.129549  [ 2700/ 4665]\n",
      "loss: 0.193860  [ 3600/ 4665]\n",
      "loss: 0.160242  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.8%, avg loss: 0.000264\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.000060  [    0/ 4665]\n",
      "loss: 0.000022  [  900/ 4665]\n",
      "loss: 0.000441  [ 1800/ 4665]\n",
      "loss: 0.001720  [ 2700/ 4665]\n",
      "loss: 0.000167  [ 3600/ 4665]\n",
      "loss: 0.007293  [ 4500/ 4665]\n",
      "\n",
      "Test Error:\n",
      "acc: 99.7%, avg loss: 0.000407\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "epochs = 15\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    train(train_dataloader, myModel, cost, optimizer)\n",
    "    test(test_dataloader, myModel)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [   ],
   "source": [
    "#testing\n",
    "myModel.eval()\n",
    "test_loss, correct = 0, 0\n",
    "reverse_class_map = ['bad', 'good']\n",
    "\n",
    "test(train_dataloader,myModel)\n",
    "test(test_dataloader,myModel)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
