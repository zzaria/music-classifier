{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchinfo import summary\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/spectrograms' #looking in subfolder train\n",
    "dataset = datasets.ImageFolder(root=data_path,transform=transforms.Compose([transforms.Grayscale(),transforms.ToTensor()]))\n",
    "class_map=dataset.class_to_idx\n",
    "print(class_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data to test and train\n",
    "#use 80% to train\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Training size:\", len(train_dataset))\n",
    "print(\"Testing size:\",len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=15,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=50,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNet (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1,self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        self.drop2 = nn.Dropout2d()\n",
    "        self.pool2=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv2, self.pool2, self.relu2,self.bn2]\n",
    "\n",
    "        # third Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        self.drop3 = nn.Dropout2d()\n",
    "        self.pool3=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv3, self.drop3, self.pool3,self.relu3,self.bn3]\n",
    "\n",
    "        # fourth Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        nn.init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        self.drop4 = nn.Dropout2d()\n",
    "        self.pool4=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv4, self.drop4, self.pool4,self.relu4,self.bn4]\n",
    "\n",
    "        # 5 Block\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        nn.init.kaiming_normal_(self.conv5.weight, a=0.1)\n",
    "        self.conv5.bias.data.zero_()\n",
    "        self.drop5 = nn.Dropout2d()\n",
    "        self.pool5=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv5, self.drop5, self.pool5,self.relu5,self.bn5]\n",
    "\n",
    "        # Block 6\n",
    "        self.conv6 = nn.Conv2d(64, 64, kernel_size=3, stride=(2, 2))\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "        nn.init.kaiming_normal_(self.conv6.weight, a=0.1)\n",
    "        self.conv6.bias.data.zero_()\n",
    "        self.drop6 = nn.Dropout2d()\n",
    "        self.pool6=nn.MaxPool2d(2)\n",
    "        conv_layers += [self.conv6, self.drop6, self.pool6,self.relu6,self.bn6]\n",
    "\n",
    "        self.flatten=nn.Flatten()\n",
    "        # Linear Classifier\n",
    "        self.lin1 = nn.Linear(19264,50)\n",
    "        self.lin2=nn.Linear(50,2)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x=self.flatten(x)\n",
    "\n",
    "        # Linear layer\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x=self.lin2(x)\n",
    "\n",
    "        # Final output\n",
    "        x=F.log_softmax(x,dim=1)\n",
    "        return x\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = CNNet()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device\n",
    "\n",
    "summary(myModel, input_size=(25,1,2813,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function used to determine best parameters\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# used to create optimal parameters\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(myModel.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the training function\n",
    "\n",
    "def train(dataloader, model, loss, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, Y) in enumerate(dataloader):\n",
    "        \n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 60 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
    "\n",
    "\n",
    "# Create the validation/test function\n",
    "\n",
    "def test(dataloader, model):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += cost(pred, Y).item()\n",
    "            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    correct /= len(dataloader.dataset)\n",
    "\n",
    "    print(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "epochs = 15\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    train(train_dataloader, myModel, cost, optimizer)\n",
    "    test(test_dataloader, myModel)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "myModel.eval()\n",
    "test_loss, correct = 0, 0\n",
    "reverse_class_map = ['bad', 'good']\n",
    "\n",
    "test(train_dataloader,myModel)\n",
    "test(test_dataloader,myModel)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "def predict(data, model):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for filename,(X, Y) in zip(data.imgs,data):\n",
    "            X.unsqueeze_(-1)\n",
    "            X=X.transpose(1,3)\n",
    "            X=X.to(device)\n",
    "            pred = model(X)\n",
    "            pred=pred.argmax(1)\n",
    "            print(filename,reverse_class_map[pred])\n",
    "            \n",
    "\n",
    "\n",
    "#convert to spectrogram first using data_prep\n",
    "inference_dataset = datasets.ImageFolder(root='./data/test/',transform=transforms.Compose([transforms.Grayscale(),transforms.ToTensor()]))\n",
    "predict(inference_dataset,myModel)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
